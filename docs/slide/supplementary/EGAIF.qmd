---
title: "階層γモデルの詳細"
format: html
editor: source
---

## モデル全体の構成

　本モデルは，階層的能動的推論モデルに感情ネットワークの更新ダイナミクスによる制御を組み合わせた，感情粒度と適応行動の計算論モデルのトイモデルである。モデルは，行動選択を行う下位層，感情の生成を行う上位層，生成された感情をもとに感情粒度パラメータを計算するネットワーク機構の3つのコンポーネントから構成される。大まかなモデルの構成としては，それぞれ次のようになる。


　まず，環境のモデルがある。本モデルにおける環境のモデルは，確率逆転学習課題を想定している。エージェントは，左と右の2つの選択肢から選択し，それによって確率的に報酬が得られる。報酬確率は一定ではなく，ある期間が経過したタイミングで逆転する。逆転する期間や具体的な報酬確率は，シミュレーションの設定によって変更される。次に，下位層のモデルがある。

　下位層は，実際に確率逆転学習課題に取り組むエージェントを表現したモデルである。エージェントは，信念の更新と行動選択を行いながら，目的(選好)を果たすように動く。信念更新の精度を制御するパラメータ$\gamma$は，上位層における感情の更新とネットワーク推定の結果によって動的に変化する。

　下位層から上位層へは，感情更新のためのシグナルが送られる。エージェントは，持っていた信念と実際の行動選択の結果によって，各感情が活性化するかしないかを能動的に推論する。感情の能動的推論による経験は，構成主義的情動理論に基づいている。このシグナルは，上位層における観測に変換される。

　上位層は，6つの感情カテゴリを持つ部分観測マルコフ決定過程(POMDP)である。上位層ではホメオスタシス(感情の恒常性)の維持を目的とするように設定しているが，トイモデルではあくまでも暫定的な設定であり，選好を変更することで上位層の目的は調整される。

　最後に，上位層において生成された感情の時系列データは，ネットワーク推定によってパラメータ化される。具体的には，時系列データに対してノードワイズ正則化ロジスティック回帰を用いることで，ネットワークの重みとノードの閾値を推定する。この計算過程は，数理的にはIsing modelと等価であり，バイナリデータの時系列データから経時的ネットワークを推定していることになる。推定された感情ネットワークは，その密度を計算する。密度は，ネットワーク内のエッジの絶対値総和である。感情粒度はネットワークの密度を用いて定量化することが可能であるという仮定の下で，1から密度を引くことにより，下位層の信念にかかる精度パラメータ$\gamma$を動的に更新する。

## 環境(Environment)

　エージェントは，2つの選択肢(Left, Right)を持つ確率逆転学習課題を行う。環境は，Leftが正解である状態1と，Rightが正解である状態2の2つの状態を持つ。また，報酬確率は，正解の選択肢の報酬確率が0.6，不正解の選択肢の報酬確率が0.4である高不確実性条件と，正解の報酬確率が0.9，不正解の報酬確率が0.1の低不確実性条件の2つの条件がある。1回のシミュレーションでは条件はどちらか一つに固定されるため，条件を変えてシミュレーションの結果が比較される。更に環境は，状態が逆転する頻度が異なる2つの条件も持つ。具体的には，100試行ごとに状態が逆転する不安定環境条件と，250試行ごとに状態が逆転する安定環境条件の2つがある。
　

## 下位層

 下位層では，エージェントは環境と直接相互作用し，信念の更新と行動選択を行う。
 
### 変数

- 状態$s^{L}:s\in[1,2]$(どちらの腕が良いかの信念)

- 行動$\upsilon^{L}: \in[1,2]$(Left, Right)

- 観測$o^{L}:o \in [get, loss]$(報酬の有無)


### 生成モデル

- A(尤度) $P(o |s)$:状態が正解なら確率0.6(安定では0.9)でget，不正解なら0.4(安定なら0.1)でget

- B(遷移行列)$P(s_t | s_{t-1},\upsilon_{t-1})$:状態は行動によって変化しないという仮定の下で単位行列に設定

- C(選好):報酬2,無報酬-4に設定し，報酬を強く選好。

- D(初期信念)：両腕共に0.5


### 推論

状態の推論では，通常のベイズ更新に加えて精度パラメータ$\gamma$による重みづけを導入した。


## シグナル


## 上位層


## ネットワーク推定